{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will the file I use to do scratch/one time use stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dck5549/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "from dateutil import parser\n",
    "from pathlib import Path\n",
    "from string import punctuation\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from 01_parse.ipynb\n",
    "\n",
    "def check_trans(word_list, messages):\n",
    "    all_words = {}\n",
    "    counter = {} # includes actual words from conversation\n",
    "    dict_counter = {} # includes words from dictionary\n",
    "    \n",
    "    for message in messages:\n",
    "        content = message.split(\" \")\n",
    "        for word in content:\n",
    "            word = word.strip(punctuation).lower()\n",
    "            if len(word)>1:\n",
    "                if word in all_words:\n",
    "                    all_words[word] += 1\n",
    "                elif word.isalpha() == True:\n",
    "                    all_words[word] = 1\n",
    "                else:\n",
    "                    if word[0].isnumeric() == False:\n",
    "                        for symbol in punctuation:\n",
    "                            if symbol in word:\n",
    "                                split_word = word.split(symbol)\n",
    "                                for section in split_word:\n",
    "                                    if len(section) > 1:\n",
    "                                        if word in all_words:\n",
    "                                            all_words[word] += 1\n",
    "                                        else:\n",
    "                                            if word.isalpha() == True:\n",
    "                                                all_words[word] = 1\n",
    "                for check in word_list:\n",
    "                    find = re.match(check, word)\n",
    "                    if find != None:\n",
    "                        if check[-1] != \"*\":\n",
    "                            if len(word) > find.span()[1]:\n",
    "                                continue\n",
    "                        else:\n",
    "                            if len(word) < len(check):\n",
    "                                continue\n",
    "                        if word not in counter:\n",
    "                            counter[word] = 1\n",
    "                        else:\n",
    "                            counter[word] += 1\n",
    "                        if check not in dict_counter:\n",
    "                            dict_counter[check] = 1\n",
    "                        else:\n",
    "                            dict_counter[check] += 1\n",
    "    return counter, dict_counter, all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from 01_parse.ipynb\n",
    "# Helper functions to parse transcripts, adapted from megan's code\n",
    "# Input: path to directory full of transcripts\n",
    "# Output: a DataFrame of all the utterances in the transcript and their speaker\n",
    "def parse_trans(path):\n",
    "\n",
    "   tran_path = Path(path)\n",
    "   tran_list = list(tran_path.glob('*.txt'))\n",
    "   transcripts = {}\n",
    "   time = 0\n",
    "   for filepath in tran_list:\n",
    "      name = Path(filepath).stem\n",
    "      with open(filepath,'r',encoding='utf-8') as my_file:\n",
    "         data = my_file.readlines()\n",
    "         clean = []\n",
    "         for line in data:\n",
    "            if \":\" in line and line[:1].isnumeric() == False:\n",
    "               clean.append(line.strip() + \"&&\" +  str(time))\n",
    "            elif \":\" in line and line[:1].isnumeric() == True:\n",
    "               time +=1\n",
    "      transcripts[name] = clean\n",
    "\n",
    "\n",
    "   last_time = 0\n",
    "   trans_df = pd.DataFrame()\n",
    "   for session in transcripts:\n",
    "      s_df = pd.DataFrame.from_dict(transcripts[session])\n",
    "      def trans_proccess(c): \n",
    "\n",
    "         global g_time\n",
    "         x=c[0].strip()\n",
    "         # print(x)\n",
    "         try:\n",
    "            speaker = re.search(r\"^[^:]*:\\s*\", x).group()[:-2]\n",
    "         except:\n",
    "            speaker = \"\"\n",
    "         try:\n",
    "            content = re.search(r\":(.*)&&\", x).group()[1:-2]\n",
    "         except:\n",
    "            content = \"\"\n",
    "         try:\n",
    "            time = int(re.search(r\"&&.*\", x).group()[2:])\n",
    "            g_time = time\n",
    "         except:\n",
    "            time = -1\n",
    "\n",
    "         if(speaker == \"\" and content == \"\"):\n",
    "            speaker = None\n",
    "            content = None\n",
    "\n",
    "         row = pd.Series(dtype='float64')\n",
    "\n",
    "         row['speaker'] = speaker\n",
    "         row['content'] = content\n",
    "         row['block'] = time-last_time \n",
    "         row['session'] = session\n",
    "         return row\n",
    "      trans_df = pd.concat([trans_df, s_df.apply(lambda x: trans_proccess(x), axis=1)])\n",
    "      last_time = g_time\n",
    "   trans_df = trans_df.dropna()\n",
    "   trans_df = trans_df[trans_df['block'] >= 0].reset_index(drop=True)\n",
    "   trans_df['block'] = (trans_df['block'] / trans_df['block'].max() * num_blocks).apply(math.floor)\n",
    "   trans_df['session'] = trans_df['session'].astype('int')\n",
    "   return trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing NEK21_MAG/13.txt so it can be hand labeled\n",
    "folder = \"temp_folder\"\n",
    "num_blocks = 3\n",
    "mag_13 = parse_trans(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = mag_13['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Data\n",
    "mag_13.to_csv(\"temp_folder/NEK21_MAG13.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I do some testing of the already trained models. Code is from 02_classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(…)cased/resolve/main/tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 222kB/s]\n",
      "(…)rt-base-uncased/resolve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 2.44MB/s]\n",
      "(…)bert-base-uncased/resolve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 67.0MB/s]\n",
      "(…)base-uncased/resolve/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 144MB/s]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "#load model\n",
    "model = BertForSequenceClassification.from_pretrained(\"../models/model_mrda_v2_t1.model\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "labels = [\"statement\", \"disruption\", \"backchannel\", \"follow-me\", \"question\"]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preproccesses data before classifcation\n",
    "def preproccess(samples):\n",
    "    encoding = tokenizer.encode_plus(samples['content'], add_special_tokens = True,\n",
    "                        max_length = 32,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                        truncation=True,\n",
    "                        padding=\"max_length\")\n",
    "  \n",
    "    samples['input_ids'] = encoding['input_ids']\n",
    "    samples['token_type_ids'] = encoding['token_type_ids']\n",
    "    samples['attention_mask'] = encoding['attention_mask']\n",
    "    return samples\n",
    "\n",
    "\n",
    "# %%\n",
    "# Uses trained model to classify\n",
    "def classify(samples):\n",
    "    out = model(samples['input_ids'], token_type_ids=samples['token_type_ids'], attention_mask=samples['attention_mask'])\n",
    "    logits = out.logits.detach().cpu().numpy()\n",
    "\n",
    "    samples['logits'] = logits[0]\n",
    "    samples['labels_h'] = labels[logits.argmax()]\n",
    "    samples['labels'] = logits.argmax()\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/625 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  4.62ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "445840"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in chat & transcript data\n",
    "NEK21_df = pd.read_csv('temp_folder/NEK21_MAG13.csv')\n",
    "NEK21_df = NEK21_df.dropna()\n",
    "\n",
    "NEK21 = Dataset.from_pandas(NEK21_df)\n",
    "\n",
    "\n",
    "\n",
    "NEK21 = NEK21.map(preproccess)\n",
    "NEK21.set_format('torch')\n",
    "NEK21 = NEK21.map(classify)\n",
    "\n",
    "NEK21.to_csv('temp_folder/NEK21_13_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in results from ^, and the hand-labeled results \n",
    "labels = [\"s\", \"d\", \"b\", \"f\", \"q\"]\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "\n",
    "preds = pd.read_csv(\"temp_folder/NEK21_13_results.csv\")\n",
    "hand_labled = pd.read_csv(\"fewshot_labels/NEK21_MAG13.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dck5549/.local/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# preproccess BERT machine labeled labels by putting them into one-hot format\n",
    "onehot_enc = OneHotEncoder(sparse=False)\n",
    "bert_labels_raw = preds['labels'][0:].to_numpy().reshape(preds['labels'].size,1)\n",
    "bert_labels = onehot_enc.fit_transform(bert_labels_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dck5549/.local/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# transform hand labels to one-hot format\n",
    "h1_raw = hand_labled\n",
    "h1_raw = h1_raw.replace({'labels_h': label2id})['labels_h'].to_numpy().reshape([-1,1])\n",
    "h1_labels = onehot_enc.fit_transform(h1_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of Hand Labels:\n",
      "s: 354\n",
      "d: 75\n",
      "b: 50\n",
      "f: 4\n",
      "q: 142\n",
      "Distribution of Model Labels:\n",
      "s: 451\n",
      "b: 12\n",
      "f: 2\n",
      "q: 160\n"
     ]
    }
   ],
   "source": [
    "#model never predicts disruption?? This is also true in the 03_bert_validation\n",
    "h,hc = np.unique(h1_raw, return_counts=True)\n",
    "b,bc = np.unique(bert_labels_raw, return_counts=True)\n",
    "\n",
    "print(\"Distribution of Hand Labels:\")\n",
    "for i, val in enumerate(h):\n",
    "    print(f'{labels[val]}: {hc[i]}')\n",
    "\n",
    "print(\"Distribution of Model Labels:\")\n",
    "for i, val in enumerate(b):\n",
    "    print(f'{labels[val]}: {bc[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accuracy: 0.7648%\n",
      "Statement Accuracy: 0.943502824858757%\n",
      "Disruption Accuracy: 0.0%\n",
      "Backchannel Accuracy: 0.12%\n",
      "Follow-me Accuracy: 0.0%\n",
      "Question Accuracy: 0.971830985915493%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "s_correct = 0\n",
    "q_correct = 0\n",
    "d_correct = 0\n",
    "b_correct = 0\n",
    "f_correct = 0\n",
    "\n",
    "\n",
    "for i,l in enumerate(h1_raw):\n",
    "\n",
    "    if l[0] == bert_labels_raw[i][0]:\n",
    "        correct += 1\n",
    "\n",
    "        t = labels[l[0]]\n",
    "        if t == \"s\":\n",
    "            s_correct += 1\n",
    "        elif t == \"q\":\n",
    "            q_correct += 1\n",
    "        elif t == \"d\":\n",
    "            d_correct += 1\n",
    "        elif t == \"f\":\n",
    "            f_correct += 1\n",
    "        else:\n",
    "            b_correct += 1\n",
    "        \n",
    "\n",
    "print(f'Total Accuracy: {correct/len(h1_raw)}%')\n",
    "print(f'Statement Accuracy: {s_correct/hc[0]}%')\n",
    "print(f'Disruption Accuracy: {d_correct/hc[1]}%')\n",
    "print(f'Backchannel Accuracy: {b_correct/hc[2]}%')\n",
    "print(f'Follow-me Accuracy: {f_correct/hc[3]}%')\n",
    "print(f'Question Accuracy: {q_correct/hc[4]}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
