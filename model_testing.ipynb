{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset swda (/home/mms9355/.cache/huggingface/datasets/swda/train/0.0.0/9af7f63229aca2a0d84408dd35ceb640b18d13f36d4b6e668f577905f6339ec0)\n",
      "100%|██████████| 3/3 [00:00<00:00, 171.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tabulate import tabulate\n",
    "from tqdm import trange\n",
    "import random\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "ds_raw = load_dataset(\"swda\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "model = BertForSequenceClassification.from_pretrained(\"model__v1_t3.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels =  [\"dummy\", \"state\", \"inform\", \"validate\", \"reject\", \"inquire\", \"direct\"]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "\n",
    "# Dictionaries that map act tags with classified act labels\n",
    "RAW_ACT_TAGS = [ 'ad', 'qo', 'qy', 'arp_nd', 'sd', 'h', 'bh', 'no', '^2', '^g', 'ar', 'aa', 'sv', 'bk', 'fp', 'qw', 'b', 'ba', 't1', 'oo_co_cc', '+', 'ny', 'qw^d', 'x', 'qh', 'fc', 'fo_o_fw_\"_by_bc', 'aap_am', '%', 'bf', 't3', 'nn', 'bd', 'ng', '^q', 'br', 'qy^d', 'fa', '^h', 'b^m', 'ft', 'qrr', 'na', ]\n",
    "ACT_LABELS = { 'sd': 1, 'b': 3, 'sv': 1, 'aa': 3, '%': 0, 'ba': 3, 'qy': 5, 'x': 0, 'ny': 3, 'fc': 1, '%': 0, 'qw': 5, 'nn': 4, 'bk': 3, 'h': 5, 'qy^d': 5, 'fo_o_fw_\"_by_bc': 0, 'bh': 5, '^q': 2, 'bf': 2, 'na': 3, 'ad': 6, '^2': 5, 'b^m': 3, 'qo': 5, 'qh': 1, '^h': 0, 'ar': 4, 'ng': 4, 'br': 4, 'no': 1, 'fp': 5, 'qrr': 5, 'arp_nd': 4, 't3': 6, 'oo_co_cc': 3, 't1': 0, 'bd': 0, 'aap_am': 3, '^g': 5, 'qw^d': 5, 'fa': 3, 'ft': 3, '+': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    }
   ],
   "source": [
    "# Encodes utterances and assigns them classified act labels\n",
    "def dataprep(samples):\n",
    "  encoding = tokenizer.encode_plus(samples['text'], add_special_tokens = True,\n",
    "                        max_length = 32,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                        truncation=True,\n",
    "                        padding=\"max_length\"\n",
    "                   )\n",
    "  samples['input_ids'] = encoding['input_ids']\n",
    "  samples['attention_masks'] = encoding['attention_mask']\n",
    "  ls = np.zeros(7)\n",
    "  ls[ACT_LABELS[RAW_ACT_TAGS[samples['damsl_act_tag']]]] = 1\n",
    "  samples['labels'] = ls\n",
    "\n",
    "  return samples\n",
    "\n",
    "# Creates encoded dataset and sets the format to pytorch\n",
    "encoded = ds_raw.map(dataprep)\n",
    "encoded.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1063,  1042,  8529,  1010,  1065,  1063,  1040,  2092,  1010,\n",
       "          1065,  2017,  5720,  2055,  1010,  1063,  1042,  7910,  1010,  1065,\n",
       "         12779,  2015,  1012,  1013,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "encoding = tokenizer.encode_plus(test, add_special_tokens = True,\n",
    "                    max_length = 32,\n",
    "                    return_attention_mask = True,\n",
    "                    return_tensors = 'pt',\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\")\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded['test'][5]['text']\n",
    "encoded['test'][5]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8367301727957466\n"
     ]
    }
   ],
   "source": [
    "corrects = 0\n",
    "for e in encoded['test']:\n",
    "    out = model(e['input_ids'], token_type_ids=None, attention_mask=e['attention_masks'])\n",
    "    logits = out.logits.detach().cpu().numpy()\n",
    "    if (np.where(e['labels'] == 1)[0][0]  == logits.argmax()):\n",
    "        corrects += 1\n",
    "accuracy = corrects/ len(encoded['test'])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not an expert. /\n",
      "Guess: state\n",
      "Actual: inquire\n"
     ]
    }
   ],
   "source": [
    "# Sample classification that doesn't work\n",
    "e = encoded['test'][100]\n",
    "print(e['text'])\n",
    "out = model(e['input_ids'], token_type_ids=None, attention_mask=e['attention_masks'])\n",
    "logits = out.logits.detach().cpu().numpy()\n",
    "print(\"Guess: \" + labels[logits.argmax()])\n",
    "print(\"Actual: \" + labels[np.where(e['labels'] == 1)[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and parts of Vermont, {F um, } they showed pictures [ of, + of ] extensive tree damage that they attributed to acid rain. /\n",
      "Guess: state\n",
      "Actual: dummy\n"
     ]
    }
   ],
   "source": [
    "# Example of flawed classification\n",
    "e = encoded['test'][122]\n",
    "print(e['text'])\n",
    "out = model(e['input_ids'], token_type_ids=None, attention_mask=e['attention_masks'])\n",
    "logits = out.logits.detach().cpu().numpy()\n",
    "print(\"Guess: \" + labels[logits.argmax()])\n",
    "print(\"Actual: \" + labels[np.where(e['labels'] == 1)[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{D well, } I found that [ they, + they ] have just a much better program than the other shows that are on T V. /\n",
      "Guess: state\n",
      "Actual: state\n"
     ]
    }
   ],
   "source": [
    "# Sample classification that works #1\n",
    "e = encoded['test'][998]\n",
    "print(e['text'])\n",
    "out = model(e['input_ids'], token_type_ids=None, attention_mask=e['attention_masks'])\n",
    "logits = out.logits.detach().cpu().numpy()\n",
    "print(\"Guess: \" + labels[logits.argmax()])\n",
    "print(\"Actual: \" + labels[np.where(e['labels'] == 1)[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Do you go through this, + is this something that you go through ] every year <laughter>. /\n",
      "Guess: inquire\n",
      "Actual: inquire\n"
     ]
    }
   ],
   "source": [
    "# Sample classification that works #2\n",
    "e = encoded['test'][775]\n",
    "print(e['text'])\n",
    "out = model(e['input_ids'], token_type_ids=None, attention_mask=e['attention_masks'])\n",
    "logits = out.logits.detach().cpu().numpy()\n",
    "print(\"Guess: \" + labels[logits.argmax()])\n",
    "print(\"Actual: \" + labels[np.where(e['labels'] == 1)[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
