{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset silicone (/home/mms9355/.cache/huggingface/datasets/silicone/mrda/1.0.0/af617406c94e3f78da85f7ea74ebfbd3f297a9665cb54adbae305b03bc4442a5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266e64eb30ef4b579b6a0e2417e4ed04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in dataset\n",
    "ds_raw = load_dataset(\"silicone\", \"mrda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in model\n",
    "model = BertForSequenceClassification.from_pretrained(\"../models/model_mrda_v2_fewshot_t1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human readable labels\n",
    "labels = [\"statement\", \"declarative question\", \"backchannel\", \"follow-me\", \"question\"]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e94ee0b67f468784d98d3726c90f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/83943 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9741622606409ea67a7ceaf33781ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b52346e2234448a02119601216f69c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15470 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encodes utterances and formats label to one-hot format\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def dataprep(samples):\n",
    "  encoding = tokenizer.encode_plus(samples['Utterance'], add_special_tokens = True,\n",
    "                        max_length = 64,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                        truncation=True,\n",
    "                        padding=\"max_length\"\n",
    "                   )\n",
    "  samples['input_ids'] = encoding['input_ids']\n",
    "  samples['attention_masks'] = encoding['attention_mask']\n",
    "  ls = np.zeros(len(labels))\n",
    "  ls[samples['Label']] = 1\n",
    "  samples['labels'] = ls\n",
    "\n",
    "  return samples\n",
    "\n",
    "# Creates encoded dataset and sets the format to pytorch\n",
    "encoded = ds_raw.map(dataprep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statement: 8864\n",
      "declarative question: 2246\n",
      "backchannel: 1961\n",
      "follow-me: 1317\n",
      "question: 1082\n"
     ]
    }
   ],
   "source": [
    "# looks at dialog act distribution of test split in dataset\n",
    "for i in range(0,len(labels)):\n",
    "    z = 0\n",
    "    for x in encoded['test']['labels']:\n",
    "        l = np.zeros(len(labels)).tolist()\n",
    "        l[i] = 1\n",
    "        if(x == l):\n",
    "            z += 1\n",
    "    print(labels[i] + \": \" + str(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format dataset for pytorch\n",
    "encoded.set_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6393018745959922"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classify test split and calculate accurary of BERT model\n",
    "corrects = 0\n",
    "i = 0\n",
    "labeled = np.zeros(len(encoded['test']))\n",
    "for e in encoded['test']:\n",
    "    out = model(e['input_ids'], token_type_ids=None, attention_mask=e['attention_masks'])\n",
    "    logits = out.logits.detach().cpu().numpy()\n",
    "    labeled[i] = logits.argmax()\n",
    "    if (np.where(e['labels'] == 1)[0][0]  == logits.argmax()):\n",
    "        corrects += 1\n",
    "    i+=1\n",
    "accuracy = corrects/ len(encoded['test'])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and um - um - next to some - some more or less bureaucratic uh - stuff with the - the data collection she's also the wizard in the data collection .\n",
      "Guess: statement\n",
      "Actual: statement\n"
     ]
    }
   ],
   "source": [
    "# Sample classification that doesn't work\n",
    "e = encoded['test'][100]\n",
    "print(e['Utterance'])\n",
    "out = model(e['input_ids'], token_type_ids=None, attention_mask=e['attention_masks'])\n",
    "logits = out.logits.detach().cpu().numpy()\n",
    "print(\"Guess: \" + labels[logits.argmax()])\n",
    "print(\"Actual: \" + labels[np.where(e['labels'] == 1)[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay | um - why don't we get started on that subject anyways ?\n",
      "Guess: question\n",
      "Actual: statement\n"
     ]
    }
   ],
   "source": [
    "# Example of flawed classification\n",
    "e = encoded['test'][122]\n",
    "print(e['Utterance'])\n",
    "out = model(e['input_ids'], token_type_ids=None, attention_mask=e['attention_masks'])\n",
    "logits = out.logits.detach().cpu().numpy()\n",
    "print(\"Guess: \" + labels[logits.argmax()])\n",
    "print(\"Actual: \" + labels[np.where(e['labels'] == 1)[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what's also nice and for a- - i- - for me in my mind .\n",
      "Guess: statement\n",
      "Actual: statement\n"
     ]
    }
   ],
   "source": [
    "# Sample classification that works #1\n",
    "e = encoded['test'][998]\n",
    "print(e['Utterance'])\n",
    "out = model(e['input_ids'], token_type_ids=None, attention_mask=e['attention_masks'])\n",
    "logits = out.logits.detach().cpu().numpy()\n",
    "print(\"Guess: \" + labels[logits.argmax()])\n",
    "print(\"Actual: \" + labels[np.where(e['labels'] == 1)[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rad !\n",
      "Guess: backchannel\n",
      "Actual: statement\n"
     ]
    }
   ],
   "source": [
    "# Sample classification that works #2\n",
    "e = encoded['test'][775]\n",
    "print(e['Utterance'])\n",
    "out = model(e['input_ids'], token_type_ids=None, attention_mask=e['attention_masks'])\n",
    "logits = out.logits.detach().cpu().numpy()\n",
    "print(\"Guess: \" + labels[logits.argmax()])\n",
    "print(\"Actual: \" + labels[np.where(e['labels'] == 1)[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statement: 13753 : oh .\n",
      "declarative question: 46 : how uh ==\n",
      "backchannel: 281 : huh .\n",
      "follow-me: 0 : __NONE__\n",
      "question: 1390 : is it under construction ?\n"
     ]
    }
   ],
   "source": [
    "# print out classified dialog act distribution\n",
    "test_df = encoded['test'].to_pandas()\n",
    "test_df['labels_pred'] = labeled\n",
    "for i in range(0, len(labels)):\n",
    "    content = \"__NONE__\"\n",
    "    df = test_df[test_df['labels_pred'] == i]['Utterance']\n",
    "    if(len(df.index) > 0):\n",
    "        rand_sample_ind = random.randint(0, len(df.index)-1)\n",
    "        if(rand_sample_ind >= 0):\n",
    "            content = df.iloc[random.randint(0, len(df.index)-1)]\n",
    "\n",
    "    print_str = labels[i] + \": \" + str(len(df.index)) + \" : \" + content\n",
    "    print(print_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
