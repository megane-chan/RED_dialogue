{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data here - should be in format like 'data/trans.csv'\n",
    "\n",
    "data_path = \"data/split_chat.csv\" #change this to whatever data u want to run through model\n",
    "preds_path = \"new_results/split_chat_preds.csv\" #this is where model preds will be saved to, change accordingly\n",
    "\n",
    "\n",
    "data_df = pd.read_csv(data_path)\n",
    "data_df['content'] = data_df['content'].astype(str) #some of the utterances werent strings? this just makes everything is a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch 0\n",
      "Starting batch 100\n",
      "Starting batch 200\n",
      "Starting batch 300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''prepare dataset, for making classifications the model will be given the statement before the statement being predicted for additional context. For example, for the following sequence:\n",
    "\n",
    "Everything is ON. I have everything.\n",
    "I have nothing. Wait, just a second. Uno momento por favor.\n",
    "Nothing [UI]\n",
    "\n",
    "To make a prediction for the first statement, the input to the model will be:\n",
    "\n",
    "\"Everything is ON. I have everything.\"\n",
    "\n",
    "For the second:\n",
    "\n",
    "\"Everything is ON. I have everything. I have nothing. Wait, just a second. Uno momento por favor\"\n",
    "\n",
    "And for the third:\n",
    "\n",
    "\"I have nothing. Wait, just a second. Uno momento por favor. Nothing [UI]\"\n",
    "\n",
    "'''\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        super(BertDataset, self).__init__()\n",
    "        self.content=data['content']\n",
    "        self.block=data['block']\n",
    "        self.session=data['session']\n",
    "        self.tokenizer=tokenizer\n",
    "        self.max_length=max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.content)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if index > 0 and self.block[index] == self.block[index - 1] and self.session[index] == self.session[index - 1]:\n",
    "            text1 = self.content[index - 1]\n",
    "            text2 = self.content[index]\n",
    "            inp = \"[CLS] \" + text1 + \"[SEP]\" + text2\n",
    "        else:\n",
    "            text = self.content[index]\n",
    "            inp = \"[CLS] \" + text + \"[SEP]\"\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            inp,\n",
    "            add_special_tokens=False,\n",
    "            return_attention_mask=True,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)\n",
    "            }\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#et up model stuff and dataset\n",
    "batch_size = 16\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"../models/mrda_backwards_context.model\")\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "dataset = BertDataset(data_df, tokenizer, max_length=256)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "#helper\n",
    "def logits_to_preds(logits):\n",
    "    max_indices = torch.argmax(logits, dim=1)\n",
    "\n",
    "    # Create a one-hot tensor\n",
    "    one_hot = torch.zeros(logits.size())\n",
    "    one_hot.scatter_(1, max_indices.view(-1, 1), 1)\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "#make predictions \n",
    "model.eval()\n",
    "preds = []\n",
    "for i, batch in enumerate(dataloader):\n",
    "\n",
    "    b_input_ids = batch['ids']\n",
    "    b_input_mask = batch['mask']\n",
    "    b_token_type_ids = batch['token_type_ids']\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        eval_output = model(b_input_ids, \n",
    "                            token_type_ids = None,\n",
    "                            attention_mask = b_input_mask)\n",
    "    \n",
    "    logits = logits_to_preds(eval_output.logits.to(torch.float))\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    preds.append(logits)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Starting batch {i}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#format predictions\n",
    "\n",
    "unpacked_preds = []\n",
    "\n",
    "m = {\n",
    "    0 : 'statement',\n",
    "    1 : 'disruption',\n",
    "    2 : 'backchannel',\n",
    "    3 : 'floor-grabber',\n",
    "    4 : 'question'\n",
    "}\n",
    "\n",
    "for batch in preds:\n",
    "    for p in batch:\n",
    "        i = np.argmax(p)\n",
    "        unpacked_preds.append(m[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preds_df = pd.DataFrame(unpacked_preds, columns=['Preds'])\n",
    "\n",
    "\n",
    "\n",
    "#save results to new csv\n",
    "preds_df.to_csv(preds_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
