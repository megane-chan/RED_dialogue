{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mms9355/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 11.5k/11.5k [00:00<00:00, 9.37MB/s]\n",
      "Downloading metadata: 100%|██████████| 7.75k/7.75k [00:00<00:00, 6.09MB/s]\n",
      "Downloading readme: 100%|██████████| 11.2k/11.2k [00:00<00:00, 5.21MB/s]\n",
      "No config specified, defaulting to: irc_disentangle/ubuntu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset irc_disentangle/ubuntu to /home/mms9355/.cache/huggingface/datasets/irc_disentangle/ubuntu/1.0.0/c67c84ac97840f40049d0530ffe40b177d8d636aa708823ace10221a59042572...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 118MB [00:09, 12.0MB/s] \n",
      "                                                                             \r"
     ]
    },
    {
     "ename": "NonMatchingSplitsSizesError",
     "evalue": "[{'expected': SplitInfo(name='train', num_bytes=56012854, num_examples=220616, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='train', num_bytes=0, num_examples=0, shard_lengths=None, dataset_name='irc_disentangle')}, {'expected': SplitInfo(name='validation', num_bytes=3081479, num_examples=12510, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='validation', num_bytes=0, num_examples=0, shard_lengths=None, dataset_name='irc_disentangle')}, {'expected': SplitInfo(name='test', num_bytes=3919900, num_examples=15010, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='test', num_bytes=0, num_examples=0, shard_lengths=None, dataset_name='irc_disentangle')}]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNonMatchingSplitsSizesError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39;49m\u001b[39mirc_disentangle\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/load.py:1791\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1788\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   1790\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1791\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m   1792\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1793\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1794\u001b[0m     verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m   1795\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[1;32m   1796\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m   1797\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   1798\u001b[0m )\n\u001b[1;32m   1800\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1801\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   1802\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   1803\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py:891\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[0;32m--> 891\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m    892\u001b[0m         dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[1;32m    893\u001b[0m         verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m    894\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs,\n\u001b[1;32m    895\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs,\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    897\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py:1651\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_download_and_prepare\u001b[39m(\u001b[39mself\u001b[39m, dl_manager, verification_mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1651\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m   1652\u001b[0m         dl_manager,\n\u001b[1;32m   1653\u001b[0m         verification_mode,\n\u001b[1;32m   1654\u001b[0m         check_duplicate_keys\u001b[39m=\u001b[39;49mverification_mode \u001b[39m==\u001b[39;49m VerificationMode\u001b[39m.\u001b[39;49mBASIC_CHECKS\n\u001b[1;32m   1655\u001b[0m         \u001b[39mor\u001b[39;49;00m verification_mode \u001b[39m==\u001b[39;49m VerificationMode\u001b[39m.\u001b[39;49mALL_CHECKS,\n\u001b[1;32m   1656\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_splits_kwargs,\n\u001b[1;32m   1657\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py:1004\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1001\u001b[0m     dl_manager\u001b[39m.\u001b[39mmanage_extracted_files()\n\u001b[1;32m   1003\u001b[0m \u001b[39mif\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS \u001b[39mor\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS:\n\u001b[0;32m-> 1004\u001b[0m     verify_splits(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo\u001b[39m.\u001b[39;49msplits, split_dict)\n\u001b[1;32m   1006\u001b[0m \u001b[39m# Update the info object with the splits.\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits \u001b[39m=\u001b[39m split_dict\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/utils/info_utils.py:100\u001b[0m, in \u001b[0;36mverify_splits\u001b[0;34m(expected_splits, recorded_splits)\u001b[0m\n\u001b[1;32m     94\u001b[0m bad_splits \u001b[39m=\u001b[39m [\n\u001b[1;32m     95\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mexpected\u001b[39m\u001b[39m\"\u001b[39m: expected_splits[name], \u001b[39m\"\u001b[39m\u001b[39mrecorded\u001b[39m\u001b[39m\"\u001b[39m: recorded_splits[name]}\n\u001b[1;32m     96\u001b[0m     \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m expected_splits\n\u001b[1;32m     97\u001b[0m     \u001b[39mif\u001b[39;00m expected_splits[name]\u001b[39m.\u001b[39mnum_examples \u001b[39m!=\u001b[39m recorded_splits[name]\u001b[39m.\u001b[39mnum_examples\n\u001b[1;32m     98\u001b[0m ]\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(bad_splits) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[39mraise\u001b[39;00m NonMatchingSplitsSizesError(\u001b[39mstr\u001b[39m(bad_splits))\n\u001b[1;32m    101\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mAll the splits matched successfully.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNonMatchingSplitsSizesError\u001b[0m: [{'expected': SplitInfo(name='train', num_bytes=56012854, num_examples=220616, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='train', num_bytes=0, num_examples=0, shard_lengths=None, dataset_name='irc_disentangle')}, {'expected': SplitInfo(name='validation', num_bytes=3081479, num_examples=12510, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='validation', num_bytes=0, num_examples=0, shard_lengths=None, dataset_name='irc_disentangle')}, {'expected': SplitInfo(name='test', num_bytes=3919900, num_examples=15010, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='test', num_bytes=0, num_examples=0, shard_lengths=None, dataset_name='irc_disentangle')}]"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('irc_disentangle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
